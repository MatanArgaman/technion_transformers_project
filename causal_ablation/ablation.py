# -*- coding: utf-8 -*-
"""Copy of TransformerLens_+_EncDec_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ndsgJH3RK41zCpmW7GIOYKZnvFWplfBr
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformer_lens

from transformer_lens import HookedEncoderDecoder
import transformer_lens.utils as utils
from transformers.models.auto.tokenization_auto import AutoTokenizer
from transformers.models.auto.modeling_auto import AutoModelForSeq2SeqLM
from transformer_lens.loading_from_pretrained import OFFICIAL_MODEL_NAMES
from transformers.models.t5.tokenization_t5_fast import T5TokenizerFast
import transformers
import json
import os
import time
import numpy as np
from typing import List, Tuple, Dict, Any, Optional, Union
import re
import random
import uuid
from tqdm import tqdm
import argparse
import torch
import logging
import datetime
import pickle

torch.set_grad_enabled(False)
# Generate random jobid before setting seed
jobid = str(uuid.uuid4())[:8]  # Use first 8 characters of UUID for shorter jobid


# Suppress warnings

def predict(query: str, tokenizer: T5TokenizerFast, model) -> Tuple[torch.Tensor, str]:
    input_tokens = tokenizer(query, return_tensors='pt')['input_ids']
    decoder_input = torch.tensor([[0]])
    logits, cache = model.run_with_cache(input_tokens, decoder_input, remove_batch_dim=True)
    return logits, tokenizer.decode(torch.argmax(logits, dim=-1)[0][0])


def result_to_docid(result: str) -> str:
    return re.sub(r'@DOC_ID_(-?\d+)@', r'\1', result)


def predict_batch(query_list: List[str], tokenizer: T5TokenizerFast, model, device, remove_batch_dim=True) -> Tuple[
    torch.Tensor, List[str], Any]:
    batch = tokenizer(query_list, return_tensors='pt', padding=True)['input_ids']
    decoder_input = torch.zeros((len(query_list), 1), dtype=torch.long, device=device)
    logits, cache = model.run_with_cache(batch, decoder_input, remove_batch_dim=remove_batch_dim)
    predictions = tokenizer.batch_decode(torch.argmax(logits, dim=-1), skip_special_tokens=True)
    del decoder_input
    torch.cuda.empty_cache()  # Clear unoccupied cache
    return logits, predictions, cache


def get_metrics(logits: List[torch.Tensor], ground_truths: List[Dict], calc_mrr=False):
    reciprocal_ranks = []
    hits_at_1_count = 0
    hits_at_5_count = 0
    hits_at_10_count = 0
    for l, gt in zip(logits, ground_truths):
        assert len(gt['relevant_logits']) == 1
        gt = gt['relevant_logits'][0]
        # Sort in descending order (highest logits first)
        y = l[0]
        logit_order = torch.argsort(y, descending=True)
        if not calc_mrr:
            logit_order = logit_order[:10]
        index = (logit_order == gt).nonzero()
        if len(index) > 0:
            rank = index[0] + 1  # 1-based rank
            reciprocal_ranks.append(1.0 / rank)
            if rank == 1:
                hits_at_1_count += 1
            if rank <= 5:
                hits_at_5_count += 1
            if rank <= 10:
                hits_at_10_count += 1
        else:
            reciprocal_ranks.append(0.0)
    if calc_mrr:
        mrr = sum(reciprocal_ranks) / len(ground_truths)
    else:
        mrr = None
    hits_at_1 = hits_at_1_count / len(ground_truths)
    hits_at_5 = hits_at_5_count / len(ground_truths)
    hits_at_10 = hits_at_10_count / len(ground_truths)
    return mrr, hits_at_1, hits_at_5, hits_at_10


def evaluate(x: List[Dict], chunk_size: int, tokenizer, model, device, calc_mrr: bool = False, threshold: float = None,
             layers: list = None) -> Tuple[
    List[torch.Tensor], Optional[float], float, float, float, Optional[Any], Optional[dict]]:
    logits = []
    first_cache = None
    activation_bools = None
    if threshold is not None and layers is not None:
        activation_bools = {layer: [] for layer in layers}
    for i in tqdm(range(0, len(x), chunk_size), desc="Evaluating"):
        q_chunk = x[i: i + chunk_size]
        queries = [q['query'] for q in q_chunk]
        l, _, cache = predict_batch(queries, tokenizer, model, device, remove_batch_dim=threshold is None)
        if i == 0:
            first_cache = cache
        logits.extend(l)
        if threshold is not None and layers is not None:
            for layer in layers:
                acts = cache[f"decoder.{layer}.mlp.hook_post"]  # (batch, neurons)
                bools = (acts >= threshold).cpu().numpy()
                activation_bools[layer].append(bools)
    mrr, hits_at_1, hits_at_5, hits_at_10 = get_metrics(logits, x, calc_mrr=calc_mrr)
    if threshold is not None and layers is not None:
        # Concatenate along batch dimension for each layer
        for layer in layers:
            activation_bools[layer] = np.concatenate(activation_bools[layer], axis=0) # (num_queries, 1, num_neurons)
            assert activation_bools[layer].shape[1]==1
            activation_bools[layer]=activation_bools[layer][:,0] # (num_queries, num_neurons)
        return logits, mrr, hits_at_1, hits_at_5, hits_at_10, first_cache, activation_bools
    else:
        return logits, mrr, hits_at_1, hits_at_5, hits_at_10, first_cache, None


def ablation_hook_factory(neuron_idx):
    def hook_fn(tensor, hook):
        tensor[..., neuron_idx] = 0
        return tensor

    return hook_fn


def save_neuron_sets(out_dir: str, ablated_neurons: list, cannot_ablate_neurons: list, neurons_to_check: list):
    print(f"Saving neuron sets to {out_dir}")
    with open(os.path.join(out_dir, "ablated_neurons.json"), "w") as f:
        json.dump(ablated_neurons, f)
    with open(os.path.join(out_dir, "cannot_ablate_neurons.json"), "w") as f:
        json.dump(cannot_ablate_neurons, f)
    with open(os.path.join(out_dir, "neurons_not_checked.json"), "w") as f:
        json.dump(neurons_to_check, f)


def check_ablated_neurons(caches: dict, ablated_neurons: list):
    # Check that all ablated neurons are zeroed in caches[0]
    for (layer, neuron_idx) in ablated_neurons:
        key = f"decoder.{layer}.mlp.hook_post"
        tensor = caches[key]
        assert torch.all(tensor[..., neuron_idx] == 0), "Neuron {(layer, neuron_idx)} is NOT zeroed!"


def check_non_ablated_neuron(caches: dict, previous_caches: dict, neuron):
    layer, neuron_idx = neuron
    key = f"decoder.{layer}.mlp.hook_post"
    current_tensor = caches[key]
    previous_tensor = previous_caches[key]
    assert ((current_tensor[..., neuron_idx] != 0) | (
            previous_tensor[..., neuron_idx] == 0)).all(), f"Neuron {(layer, neuron_idx)} is zeroed!"


def get_current_time_str():
    now = datetime.datetime.now()
    time_str = now.strftime("%d_%m__%H_%M_%S")
    return time_str


def sanity_checks(train, tokenizer, model, device):
    # evaluate on training set to see model indeed returns correct value:
    # we see that we need another function to match values
    for s_num in range(2):
        print(train[s_num]['relevant_docs'], predict(train[s_num]['query'], tokenizer, model)[1])
    # validate predict_batch function:

    total_queries = 2
    start_time = time.time()
    queries = [q['query'] for q in train[:total_queries]]
    logits, predictions, cache = predict_batch(queries, tokenizer, model, device)
    top_1 = torch.argmax(logits, dim=2)
    for s_num in range(total_queries):
        if not (top_1[s_num][0] == train[s_num]['relevant_logits'][0]):
            print(s_num)
            print(top_1[s_num][0], train[s_num]['relevant_logits'][0])

        if not (eval(result_to_docid(predictions[s_num])) == train[s_num]['relevant_docs'][0]):
            print(s_num)
            print(train[s_num]['relevant_docs'], result_to_docid(predict(train[s_num]['query'], tokenizer, model)[1]))
    total_time = time.time() - start_time
    print(f'single sample query prediction time: {int(total_time)} seconds')

    chunk_size = 10
    start_time = time.time()
    for i in range(0, total_queries, chunk_size):
        q_chunk = train[i: i + chunk_size]
        queries = [q['query'] for q in q_chunk]
        logits, predictions, cache = predict_batch(queries, tokenizer, model, device)
    total_time = time.time() - start_time
    print(f'batch query prediction time: {int(total_time)} seconds')


def add_logit_gt(x: List[Dict], doc_to_logit: dict) -> None:
    for sample in x:
        sample['relevant_logits'] = []
        for docid in sample['relevant_docs']:
            sample['relevant_logits'].append(doc_to_logit[str(docid)])


def filter_on_topic(x: List[Dict], docs: List[Dict], topics_path: str, topic_index: int, out_dir: Optional[str], topic_id=None) -> List[Dict]:
    with open(os.path.join(topics_path, 'topics.pkl'), 'rb') as fp:
        data = pickle.load(fp)
    if topic_id is None:
        selected = np.load(os.path.join(topics_path, 'selected.npy'))
        topic_id = selected[topic_index]
    indices = set(np.where(np.array(data['topics']) == topic_id)[0])
    doc_set = set([d['id'] for i, d in enumerate(docs) if i in indices])
    y = [a for a in x if a['relevant_docs'][0] in doc_set]

    topic_name = data['dataframe'][data['dataframe']['Topic'] == topic_id]['Name'].item()
    if out_dir:
        with open(os.path.join(out_dir, 'topic.json'), 'w') as fp:
            json.dump({'topic_id': int(topic_id), 'topic_name': topic_name}, fp)
    return y


def get_doc_to_logit(tokenizer, tokenizer_t5):
    # Our model has a new token for each document id that we trained it on.
    # token id of first document that was added
    first_added_doc_id = len(tokenizer_t5)
    # token id of the last document that was added
    last_added_doc_id = len(tokenizer_t5) + (len(tokenizer) - len(tokenizer_t5))
    tokens_by_tokenization_order = tokenizer.batch_decode(np.arange(first_added_doc_id, last_added_doc_id))
    doc_to_logit = dict(
        [(result_to_docid(r), i + first_added_doc_id) for (i, r) in enumerate(tokens_by_tokenization_order)])
    return doc_to_logit, first_added_doc_id, last_added_doc_id


def handle_cannot_ablate_neuron(neuron, ablated_neurons, cannot_ablate_neurons, model):
    ablated_neurons.remove(neuron)
    cannot_ablate_neurons.append(neuron)
    layer, neuron_idx = neuron
    hook_name = f"decoder.{layer}.mlp.hook_post"
    # Remove the hook
    model.hook_dict[hook_name].fwd_hooks[-1].hook.remove()
    model.hook_dict[hook_name].fwd_hooks.pop()


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.CRITICAL)
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint", type=str, default="C:\\projects\\transformers\\236004-HW1-GPT\\DSI-large-7423")
    parser.add_argument("--path_to_data", type=str, default="C:\\projects\\transformers\\236004-HW1-GPT\\NQ10k\\")
    parser.add_argument("--topics_path", type=str,
                        help='path to topics.pkl, selected.npy (expected in the same path)')  # "C:\\Users\\Administrator\\OneDrive\\Documents\\university\\transformers"
    parser.add_argument("--topics_id", type=int,
                        help='integer between 0 to the number of selected topics - 1, this is an index of the selected topic used for evaluation ')  # "C:\\Users\\Administrator\\OneDrive\\Documents\\university\\transformers"
    parser.add_argument("--out_dir", type=str, default="C:\\projects\\transformers\\236004-HW1-GPT\\")
    parser.add_argument("--jobid", type=str, default=jobid, help="Job ID for this run (default: random)")
    parser.add_argument("--chunk_size", type=int, default=100, help="Chunk size for evaluation (default: 100)")
    parser.add_argument("--use_validation_set", action="store_true",
                        help="Use validation set instead of test set for evaluation")
    parser.add_argument("--threshold", type=float, default=None,
                        help="Threshold for neuron activation (if set, use activation-based ablation logic)")
    parser.add_argument("--num_of_queries", type=int, default=-1,
                        help="Number of queries to use for evaluation (default: -1, use all queries)")

    args = parser.parse_args()
    checkpoint = args.checkpoint
    path_to_data = args.path_to_data
    threshold = args.threshold
    # Create out_dir with jobid prefix: jobid__dd_mm__hh_mm_ss
    time_str = get_current_time_str()
    # Add prefix based on evaluation dataset
    dataset_prefix = "v_" if args.use_validation_set else "t_"
    out_dir = os.path.join(args.out_dir, f"{dataset_prefix}{args.jobid}__{time_str}")

    # Add a folder with the jobid and current time in the format jobid__dd_mm__hh_mm_ss
    os.makedirs(out_dir, exist_ok=True)
    print(f"Job ID: {args.jobid}")
    print(f"Results will be saved in: {out_dir}")

    OFFICIAL_MODEL_NAMES.append(checkpoint)
    hf_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    device = str(utils.get_device())
    model = HookedEncoderDecoder.from_pretrained(checkpoint, hf_model=hf_model, device=device)
    tokenizer_t5 = AutoTokenizer.from_pretrained('google-t5/t5-large')
    doc_to_logit, first_added_doc_id, last_added_doc_id = get_doc_to_logit(tokenizer, tokenizer_t5)
    del tokenizer_t5
    print(first_added_doc_id, last_added_doc_id)
    query = "test query"
    input_tokens = tokenizer(query, return_tensors='pt')['input_ids']
    decoder_input = torch.tensor([[0]])
    # logits, cache = model.run_with_cache(input_tokens, decoder_input, remove_batch_dim=True)
    # Prediction from the logits
    # torch.argmax(logits, dim=-1), tokenizer.decode(torch.argmax(logits, dim=-1)[0][0])

    with open(os.path.join(path_to_data, 'documents-10000-7423.json')) as fp:
        docs = json.load(fp)
    with open(os.path.join(path_to_data, 'train_queries-10000-7423.json')) as fp:
        train = json.load(fp)
    with open(os.path.join(path_to_data, 'val_queries-10000-7423.json')) as fp:
        val = json.load(fp)
    with open(os.path.join(path_to_data, 'test_queries-10000-7423.json')) as fp:
        test = json.load(fp)


    add_logit_gt(train, doc_to_logit)
    add_logit_gt(val, doc_to_logit)
    add_logit_gt(test, doc_to_logit)

    sanity_checks(train, tokenizer, model, device)

    # Choose evaluation dataset based on flag
    eval_dataset = val if args.use_validation_set else test
    dataset_name = "validation" if args.use_validation_set else "test"
    print(f"Using {dataset_name} set for evaluation")

    if args.topics_path:
        eval_dataset = filter_on_topic(eval_dataset, docs, args.topics_path, args.topics_id, out_dir)

    # Randomly sample queries if num_of_queries is specified
    if args.num_of_queries != -1:
        if args.num_of_queries > len(eval_dataset):
            print(f"Warning: num_of_queries ({args.num_of_queries}) is larger than available queries ({len(eval_dataset)}). Using all available queries.")
            args.num_of_queries = len(eval_dataset)
        else:
            print(f"Randomly sampling {args.num_of_queries} queries from {len(eval_dataset)} available queries")
            eval_dataset = random.sample(eval_dataset, args.num_of_queries)
        
        # Save num_of_queries value to JSON file
        with open(os.path.join(out_dir, 'num_of_queries.json'), 'w') as fp:
            json.dump({'num_of_queries': args.num_of_queries}, fp)

    # If both use_validation_set and threshold are set, split eval_dataset into ablation and holdout sets
    use_holdout_split = args.use_validation_set and (threshold is not None)
    if use_holdout_split:
        # Shuffle and split
        random.shuffle(eval_dataset)
        split_idx = int(0.8 * len(eval_dataset))
        ablation_set = eval_dataset[:split_idx]
        holdout_set = eval_dataset[split_idx:]
        print(f"Ablation will be performed on {len(ablation_set)} samples, holdout set has {len(holdout_set)} samples.")
    else:
        ablation_set = eval_dataset
        holdout_set = None

    # Save threshold value if threshold is not None
    if threshold is not None:
        with open(os.path.join(out_dir, 'threshold.json'), 'w') as fp:
            json.dump({'threshold': threshold}, fp)

    # Define the layers to ablate
    STAGE3 = [18, 19, 20, 21, 22, 23]

    # Save the original hits@1 for comparison
    if threshold is None:
        original_logits, original_mrr, original_hits_at_1, _, _, previous_caches, _ = evaluate(ablation_set,
                                                                                               args.chunk_size,
                                                                                               tokenizer, model, device)
        print(f"Original HITS@1: {original_hits_at_1}")
    else:
        # Calculate original neuron activations for each neuron: which queries activate it (>= threshold)
        _, _, _, _, _, previous_caches, activation_bools = evaluate(ablation_set, args.chunk_size, tokenizer, model, device,
                                                                    threshold=threshold, layers=STAGE3)
        neuron_query_activations = {}  # (layer, neuron_idx) -> set(query_idx)
        query_count = len(ablation_set)
        for layer in STAGE3:
            bools = activation_bools[layer]  # shape: (num_queries, num_neurons)
            for neuron_idx in range(bools.shape[1]):
                active_queries = set(np.where(bools[:, neuron_idx])[0])
                neuron_query_activations[(layer, neuron_idx)] = active_queries

    # Prepare neuron lists
    neurons_to_check = []
    ablated_neurons = []
    cannot_ablate_neurons = []
    for layer in STAGE3:
        for neuron_idx in range(previous_caches[f"decoder.{layer}.mlp.hook_post"].shape[1 if threshold is None else 2]):
            neurons_to_check.append((layer, neuron_idx))

    progress_bar = tqdm(total=len(neurons_to_check), desc="Ablating neurons")
    checked_count = 0
    while neurons_to_check:
        # Randomly select a neuron to ablate
        neuron = random.sample(neurons_to_check, 1)[0]
        layer, neuron_idx = neuron
        hook_name = f"decoder.{layer}.mlp.hook_post"
        model.hook_dict[hook_name].add_hook(ablation_hook_factory(neuron_idx))
        logits, _, hits_at_1, _, _, caches, activation_bools = evaluate(ablation_set, args.chunk_size, tokenizer, model,
                                                                        device, threshold=threshold, layers=STAGE3)
        ablated_neurons.append(neuron)
        check_ablated_neurons(caches, ablated_neurons)
        cannot_ablate_current_neuron = False
        if threshold is None:
            if hits_at_1 >= original_hits_at_1:
                print(f"Neuron {neuron} can be ablated (HITS@1: {hits_at_1})")
            else:
                cannot_ablate_current_neuron = True
                handle_cannot_ablate_neuron(neuron, ablated_neurons, cannot_ablate_neurons, model)
                print(f"Neuron {neuron} cannot be ablated (HITS@1: {hits_at_1})")
        else:
            # Activation-based ablation logic
            bools = activation_bools[layer]  # shape: (num_queries, num_neurons)
            active_queries = set(np.where(bools[:, neuron_idx])[0])
            if active_queries == neuron_query_activations[neuron]:
                print(f"Neuron {neuron} can be ablated (activation pattern unchanged)")
            else:
                cannot_ablate_current_neuron = True
                handle_cannot_ablate_neuron(neuron, ablated_neurons, cannot_ablate_neurons, model)
                print(f"Neuron {neuron} cannot be ablated (activation pattern changed)")
        neurons_to_check.remove(neuron)
        progress_bar.update(1)
        checked_count += 1
        if checked_count % 100 == 0:
            save_neuron_sets(out_dir, ablated_neurons, cannot_ablate_neurons, neurons_to_check)
        previous_caches = caches
    save_neuron_sets(out_dir, ablated_neurons, cannot_ablate_neurons, neurons_to_check)
    progress_bar.close()
    print(f"Total ablated neurons: {len(ablated_neurons)}")
    print(f"Total cannot ablate neurons: {len(cannot_ablate_neurons)}")
    print(f"Neurons not checked: {len(neurons_to_check)}")

    # If using holdout split, evaluate on the holdout set after ablation
    if use_holdout_split and holdout_set is not None and len(holdout_set) > 0:
        print(f"Evaluating on holdout set of {len(holdout_set)} samples after ablation...")
        _, _, holdout_hits_at_1, _, _, _, _ = evaluate(holdout_set, args.chunk_size, tokenizer, model, device, threshold=threshold, layers=STAGE3)
        print(f"Holdout HITS@1 after ablation: {holdout_hits_at_1}")
        with open(os.path.join(out_dir, 'holdout_hits_at_1.json'), 'w') as fp:
            json.dump({'holdout_hits_at_1': holdout_hits_at_1}, fp)
